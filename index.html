<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>Visual Concepts Workshop</title>
  <style>
    body {
      margin: 0;
      font-family: 'Georgia', serif;
      color: #111;
      line-height: 1.6;
      background-color: #fdfdfd;
    }

    
    .hero {
      background-color: black;
      background-image: url('assets/bg.png');
      background-repeat: no-repeat;
      background-position: center center;
      background-size: contain;       /* 保证完整显示不裁剪 */
      aspect-ratio: 3 / 1;            /* 根据宽度自动调整高度 */
      width: 100%;
      position: relative;
      display: flex;
      align-items: center;
      justify-content: center;
      text-align: center;
      color: white;
    }

    @media (max-width: 600px) {
      .hero {
        aspect-ratio: auto;
        height: auto;
        padding-top: 33.33%;  /* fallback: 3:1 的比例高度 */
      }
    }

    .overlay {
      background-color: rgba(0, 0, 0, 0);
      width: 100%;
      height: 100%;
      position: absolute;
      top: 0;
      left: 0;
    }

    .logo-left,
    .logo-right {
      position: absolute;
      top: 20px;
      z-index: 3;
    }

    .logo-left {
      left: 20px;
    }

    .logo-right {
      right: 20px;
    }

    .logo-left img,
    .logo-right img {
      height: 60px;
      width: auto;
    }

    .hero-content {
      position: relative;
      z-index: 2;
      color: white;
    }

    .hero h1 {
      font-size: 2.8em;
      margin: 0;
    }

    .hero h2 {
      font-weight: normal;
      font-size: 1.4em;
      margin-top: 10px;
    }

    .container {
      max-width: 800px;
      margin: 40px auto;
      padding: 0 20px;
    }

    nav {
      text-align: center;
      margin: 30px 0;
    }

    nav a {
      margin: 0 12px;
      text-decoration: none;
      color: #0056b3;
    }

    section {
      margin-bottom: 60px;
    }

    h3 {
      border-bottom: 1px solid #ddd;
      padding-bottom: 5px;
      font-size: 1.2em;
    }

    a {
      color: #0056b3;
    }

    footer {
      text-align: center;
      font-size: 0.9em;
      color: #888;
      margin-top: 60px;
      padding-bottom: 40px;
    }
  </style>
</head>
<body>
  <div class="hero">
    <div class="overlay"></div>

    <!-- <div class="logo-left">
      <img src="assets/CVPR.png" alt="CVPR Logo">
    </div>

    <div class="logo-right">
      <img src="assets/MMLab.png" alt="S-Lab Logo">
    </div>

    <div class="hero-content">
      <h1>From Video Generation to World Model</h1>
      <h2>CVPR 2025 Tutorial • Nashville, USA</h2>
    </div> -->
  </div>

  <div class="container">
    <nav>
      <a href="#about">About</a>
      <!-- <a href="#cfp">Call for Papers</a> -->
      <a href="#speakers">Speakers</a>
      <a href="#schedule">Schedule</a>
      <a href="#organizers">Organizers</a>
    </nav>

    <section id="about">
        <h3>About</h3>
        <p>
          In recent years, the research community has made significant strides in generative models, particularly in the area of video generation. Despite challenges in generating temporally coherent and physically realistic videos, recent breakthroughs such as SORA, Genie, and MovieGen show promising progress toward controllable, high-fidelity visual world models. This tutorial offers a deep dive into recent advances in text-to-video generation, diffusion-based video models, and the bridge from generative video to physical and interactive world modeling. We aim to provide attendees with a comprehensive understanding of these cutting-edge methods and how they contribute to building embodied world models.
        </p>
      </section>
      
      <section id="speakers">
        <h3>Invited Speakers</h3>
        <ul>
          <li>Angjoo Kanazawa (UC Berkeley)</li>
          <li>Jiaming Song (Luma AI)</li>
          <li>Xintao Wang (KwaiVGI)</li>
          <li>Jack Parker-Holder (DeepMind)</li>
          <li>Tim Brooks (DeepMind)</li>
          <li>Bill Peebles (OpenAI) <em>(Tentative)</em></li>
          <li>Chenlin Meng (Stanford / Pika) <em>(Tentative)</em></li>
          <li>Yuge (Jimmy) Shi (DeepMind) <em>(Tentative)</em></li>
        </ul>
      </section>
      
      <section id="schedule">
        <h3>Schedule</h3>
        <ul>
          <li><strong>08:30 – 08:40</strong> Opening Remarks</li>
          <li><strong>08:40 – 09:30</strong> Invited Talk 1</li>
          <li><strong>09:30 – 10:20</strong> Invited Talk 2</li>
          <li><strong>10:20 – 10:40</strong> Coffee Break</li>
          <li><strong>10:40 – 11:30</strong> Invited Talk 3</li>
          <li><strong>12:00 – 13:30</strong> Lunch Break</li>
          <li><strong>13:30 – 14:20</strong> Invited Talk 4</li>
          <li><strong>14:20 – 15:10</strong> Invited Talk 5</li>
          <li><strong>15:10 – 15:30</strong> Coffee Break</li>
          <li><strong>15:30 – 16:20</strong> Invited Talk 6</li>
          <li><strong>16:20 – 17:10</strong> Invited Talk 7</li>
          <li><strong>17:10 – 18:20</strong> Invited Talk 8</li>
          <li><strong>18:20 – 18:30</strong> Ending Remarks</li>
        </ul>
      </section>
      
      <section id="organizers">
        <h3>Organizers</h3>
        <ul>
          <li>Ziwei Liu (Associate Professor, MMLab, Nanyang Technological University)</li>
          <li>Zhaoxi Chen (Ph.D. Student, MMLab, Nanyang Technological University)</li>
          <li>Weichen Fan (Ph.D. Student, MMLab, Nanyang Technological University)</li>
        </ul>
      </section>
      



    <footer>
      &copy; From Video Generation to World Model Tutorial
    </footer>
  </div>
</body>
</html>
